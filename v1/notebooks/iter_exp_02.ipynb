{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T20:07:42.163134Z",
     "start_time": "2023-06-08T20:07:40.479074Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoking __init__.py for NDNT.utils\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# not best practice, but makes it easier to import from subdirectory\n",
    "sys.path.insert(0, './lib')\n",
    "\n",
    "import experiment as exp\n",
    "import model as m\n",
    "import plot\n",
    "import predict\n",
    "import runner2 as r\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "import umap\n",
    "import matplotlib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "_ = plt.rcParams['axes.grid'] = False # turn off grid\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T20:26:56.027274Z",
     "start_time": "2023-06-08T20:26:56.008027Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tconv_scaffold_iter_expanded():\n",
    "    # Temporal Convolutional Scaffold with Iterative Layer\n",
    "    tconv_layer = m.TemporalConvolutionalLayer(\n",
    "        num_filters=2,\n",
    "        num_inh=1,\n",
    "        filter_dims=[1, 17, 1, 11],\n",
    "        window='hamming',\n",
    "        NLtype=m.NL.relu,\n",
    "        norm_type=m.Norm.unit,\n",
    "        bias=False,\n",
    "        initialize_center=True,\n",
    "        output_norm='batch',\n",
    "        padding='spatial',\n",
    "        reg_vals={'d2xt': 0.0001,\n",
    "                  'center': 0,\n",
    "                  'bcs': {'d2xt': 1}})\n",
    "\n",
    "    proj_layer = m.TemporalConvolutionalLayer(\n",
    "        num_filters=16,\n",
    "        num_inh=8,\n",
    "        filter_dims=[2, 17, 1, 1], # this is because the input coming to this layer is 2D\n",
    "        NLtype=m.NL.linear,\n",
    "        bias=False,\n",
    "        initialize_center=False)\n",
    "\n",
    "    itert_layer = m.IterativeTemporalConvolutionalLayer(\n",
    "        num_filters=16,\n",
    "        num_inh=8,\n",
    "        filter_dims=7,\n",
    "        num_lags=2,\n",
    "        window='hamming',\n",
    "        NLtype=m.NL.relu,\n",
    "        norm_type=m.Norm.unit,\n",
    "        bias=False,\n",
    "        initialize_center=True,\n",
    "        output_norm='batch',\n",
    "        num_iter=3,\n",
    "        output_config='full',\n",
    "        reg_vals={'d2xt': 0.0001,\n",
    "                  'center': 0,\n",
    "                  'bcs': {'d2xt': 1}})\n",
    "\n",
    "    readout_layer = m.Layer(\n",
    "        # because we have inhibitory subunits on the first layer\n",
    "        pos_constraint=True,\n",
    "        norm_type=m.Norm.none,\n",
    "        NLtype=m.NL.softplus,\n",
    "        bias=True,\n",
    "        initialize_center=True)\n",
    "\n",
    "    inp_stim = m.Input(covariate='stim', input_dims=[1,36,1,14])\n",
    "\n",
    "    core_net = m.Network(layers=[tconv_layer, proj_layer, itert_layer],\n",
    "                         network_type=m.NetworkType.scaffold,\n",
    "                         name='core')\n",
    "    readout_net = m.Network(layers=[readout_layer],\n",
    "                            name='readout')\n",
    "    # this is set as a starting point, but updated on each iteration\n",
    "    output_11 = m.Output(num_neurons=11)\n",
    "\n",
    "    inp_stim.to(core_net)\n",
    "    core_net.to(readout_net)\n",
    "    readout_net.to(output_11)\n",
    "    itert_model = m.Model(output_11,\n",
    "                          name='TconvScaffoldIter',\n",
    "                          create_NDN=True, verbose=True)\n",
    "    return itert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T20:26:56.774956Z",
     "start_time": "2023-06-08T20:26:56.750918Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MODEL ===\n",
      "--- core ---\n",
      "{'ffnet_n': None,\n",
      " 'ffnet_type': 'scaffold',\n",
      " 'layer_list': [{'NLtype': 'relu',\n",
      "                 'bias': False,\n",
      "                 'bias_initializer': 'zeros',\n",
      "                 'conv_dims': None,\n",
      "                 'dilation': 1,\n",
      "                 'filter_dims': [1, 17, 1, 11],\n",
      "                 'folded_lags': False,\n",
      "                 'initialize_center': True,\n",
      "                 'input_dims': [1, 36, 1, 14],\n",
      "                 'layer_type': 'tconv',\n",
      "                 'norm_type': 1,\n",
      "                 'num_filters': 2,\n",
      "                 'num_inh': 1,\n",
      "                 'output_norm': 'batch',\n",
      "                 'padding': 'spatial',\n",
      "                 'pos_constraint': False,\n",
      "                 'reg_vals': {'bcs': {'d2xt': 1}, 'center': 0, 'd2xt': 0.0001},\n",
      "                 'res_layer': False,\n",
      "                 'stride': 1,\n",
      "                 'temporal_tent_spacing': 1,\n",
      "                 'weights_initializer': 'xavier_uniform',\n",
      "                 'window': 'hamming'},\n",
      "                {'NLtype': 'lin',\n",
      "                 'bias': False,\n",
      "                 'bias_initializer': 'zeros',\n",
      "                 'conv_dims': None,\n",
      "                 'dilation': 1,\n",
      "                 'filter_dims': [2, 17, 1, 1],\n",
      "                 'folded_lags': False,\n",
      "                 'initialize_center': False,\n",
      "                 'input_dims': None,\n",
      "                 'layer_type': 'tconv',\n",
      "                 'norm_type': 0,\n",
      "                 'num_filters': 16,\n",
      "                 'num_inh': 8,\n",
      "                 'output_norm': None,\n",
      "                 'padding': 'spatial',\n",
      "                 'pos_constraint': False,\n",
      "                 'reg_vals': {},\n",
      "                 'res_layer': False,\n",
      "                 'stride': 1,\n",
      "                 'temporal_tent_spacing': 1,\n",
      "                 'weights_initializer': 'xavier_uniform',\n",
      "                 'window': None},\n",
      "                {'NLtype': 'relu',\n",
      "                 'bias': False,\n",
      "                 'bias_initializer': 'zeros',\n",
      "                 'conv_dims': None,\n",
      "                 'filter_width': 7,\n",
      "                 'folded_lags': False,\n",
      "                 'initialize_center': True,\n",
      "                 'input_dims': None,\n",
      "                 'layer_type': 'iterT',\n",
      "                 'norm_type': 1,\n",
      "                 'num_filters': 16,\n",
      "                 'num_inh': 8,\n",
      "                 'num_iter': 3,\n",
      "                 'num_lags': 2,\n",
      "                 'output_config': 'full',\n",
      "                 'output_norm': 'batch',\n",
      "                 'pos_constraint': False,\n",
      "                 'reg_vals': {'bcs': {'d2xt': 1}, 'center': 0, 'd2xt': 0.0001},\n",
      "                 'res_layer': True,\n",
      "                 'temporal_tent_spacing': 1,\n",
      "                 'weights_initializer': 'xavier_uniform',\n",
      "                 'window': 'hamming'}],\n",
      " 'xstim_n': 'stim'}\n",
      "--- readout ---\n",
      "{'ffnet_n': [0],\n",
      " 'ffnet_type': 'normal',\n",
      " 'layer_list': [{'NLtype': 'softplus',\n",
      "                 'bias': True,\n",
      "                 'bias_initializer': 'zeros',\n",
      "                 'initialize_center': True,\n",
      "                 'input_dims': None,\n",
      "                 'layer_type': 'normal',\n",
      "                 'norm_type': 0,\n",
      "                 'num_filters': 11,\n",
      "                 'num_inh': 0,\n",
      "                 'output_norm': None,\n",
      "                 'pos_constraint': True,\n",
      "                 'reg_vals': {},\n",
      "                 'weights_initializer': 'xavier_uniform'}],\n",
      " 'xstim_n': None}\n"
     ]
    }
   ],
   "source": [
    "model = tconv_scaffold_iter_expanded()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T20:26:20.419266Z",
     "start_time": "2023-06-08T20:26:13.345016Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lazy loading dataset\n",
      "Loading expt04\n",
      "  Time embedding...\n",
      "  Done.\n",
      "Loading expt06\n",
      "  Time embedding...\n",
      "  Done.\n",
      "Loading expt07\n",
      "  Time embedding...\n",
      "  Done.\n",
      "Loading expt09\n",
      "  Time embedding...\n",
      "  Done.\n",
      "Loading expt11\n",
      "  Time embedding...\n",
      "  Done.\n"
     ]
    }
   ],
   "source": [
    "e = exp.load('iter_exps07_2iters', experiment_location='experiments', datadir='Mdata')\n",
    "dataset = e.trials[0].dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T20:26:31.152332Z",
     "start_time": "2023-06-08T20:26:31.132583Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network 0:\n",
      "  Layer 0:\n",
      "      weight: torch.Size([187, 2])\n",
      "  Layer 1:\n",
      "      weight: torch.Size([17, 16])\n",
      "  Layer 2:\n",
      "      weight: torch.Size([224, 16])\n",
      "Network 1:\n",
      "  Layer 0:\n",
      "      weight: torch.Size([2376, 11])\n",
      "      bias: torch.Size([11])\n"
     ]
    }
   ],
   "source": [
    "model.NDN.list_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T20:26:32.010364Z",
     "start_time": "2023-06-08T20:26:31.987063Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 36, 1, 14], [2, 36, 1, 4])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.NDN.networks[0].layers[0].input_dims, model.NDN.networks[0].layers[0].output_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T20:26:32.800050Z",
     "start_time": "2023-06-08T20:26:32.780859Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, 36, 1, 4], [16, 36, 1, 4])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.NDN.networks[0].layers[1].input_dims, model.NDN.networks[0].layers[1].output_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T20:26:32.951653Z",
     "start_time": "2023-06-08T20:26:32.918645Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([16, 36, 1, 4], [48, 36, 1, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.NDN.networks[0].layers[2].input_dims, model.NDN.networks[0].layers[2].output_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T20:26:33.129207Z",
     "start_time": "2023-06-08T20:26:33.070660Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([66, 36, 1, 1], [48, 36, 1, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.NDN.networks[1].layers[0].input_dims, model.NDN.networks[0].layers[2].output_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T20:26:33.354028Z",
     "start_time": "2023-06-08T20:26:33.220317Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.Size([10, 504]) w torch.Size([2, 1, 17, 11]) s torch.Size([10, 1, 52, 14])\n",
      "x torch.Size([10, 288]) w torch.Size([16, 1, 17, 1]) s torch.Size([10, 2, 52, 4])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [16, 1, 17, 1], expected input[10, 2, 52, 4] to have 1 channels, but got 2 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNDN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/mattjac/v1/NDNT/NDNT.py:170\u001b[0m, in \u001b[0;36mNDN.forward\u001b[0;34m(self, Xs)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, Xs):\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;124;03m\"\"\"This applies the forwards of each network in sequential order.\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m    The tricky thing is concatenating multiple-input dimensions together correctly.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03m    Note that the external inputs is actually in principle a list of inputs\"\"\"\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m     net_ins, net_outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_network_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mXs\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# For now assume its just one output, given by the first value of self.ffnet_out\u001b[39;00m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m net_outs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffnet_out[\u001b[38;5;241m0\u001b[39m]]\n",
      "File \u001b[0;32m~/projects/mattjac/v1/NDNT/NDNT.py:152\u001b[0m, in \u001b[0;36mNDN.compute_network_outputs\u001b[0;34m(self, Xs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetworks[ii]\u001b[38;5;241m.\u001b[39mffnets_in \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;66;03m# then getting external input\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     net_ins\u001b[38;5;241m.\u001b[39mappend( [Xs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetworks[ii]\u001b[38;5;241m.\u001b[39mxstim_n]] )\n\u001b[0;32m--> 152\u001b[0m     net_outs\u001b[38;5;241m.\u001b[39mappend( \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetworks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mii\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet_ins\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m )\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     in_nets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetworks[ii]\u001b[38;5;241m.\u001b[39mffnets_in\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/mattjac/v1/NDNT/networks.py:333\u001b[0m, in \u001b[0;36mScaffoldNetwork.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    330\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_input(inputs)\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 333\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m     nt \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m layer\u001b[38;5;241m.\u001b[39moutput_dims[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_lags_out:\n\u001b[1;32m    336\u001b[0m         \u001b[38;5;66;03m# Need to return just first lag (lag0) -- 'chomp'\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/mattjac/v1/NDNT/modules/layers/convlayers.py:534\u001b[0m, in \u001b[0;36mTconvLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    530\u001b[0m         s \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(s, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npads, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, w\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m, s\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 534\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m        \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m        \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    542\u001b[0m     w \u001b[38;5;241m=\u001b[39m w\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_dims \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_filters])\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m) \u001b[38;5;66;03m# [C,H,W,T,N]->[N,C,H,W,T]\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [16, 1, 17, 1], expected input[10, 2, 52, 4] to have 1 channels, but got 2 channels instead"
     ]
    }
   ],
   "source": [
    "model.NDN(dataset[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T20:27:28.260141Z",
     "start_time": "2023-06-08T20:27:28.259296Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.Size([30, 504]) w torch.Size([2, 1, 17, 11]) s torch.Size([30, 1, 52, 14])\n",
      "x torch.Size([30, 288]) w torch.Size([16, 2, 17, 1]) s torch.Size([30, 2, 52, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([30, 288]), torch.Size([30, 2304]), torch.Size([30, 1728]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = model.NDN.networks[0].layers[0](dataset[:30]['stim'])\n",
    "b = model.NDN.networks[0].layers[1](a)\n",
    "c = model.NDN.networks[0].layers[2](b)\n",
    "a.shape, b.shape, c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T20:18:56.986479Z",
     "start_time": "2023-06-08T20:18:56.902328Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 8, 8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.NDN.networks[0].layers[1]._npads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
